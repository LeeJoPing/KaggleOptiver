{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1bad271-c163-4c3f-8e40-ebe5332e0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gc \n",
    "import os  \n",
    "import time  \n",
    "import warnings  \n",
    "from itertools import combinations \n",
    "from warnings import simplefilter  \n",
    "import joblib  \n",
    "import lightgbm as lgb  \n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn.metrics import mean_absolute_error  \n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit  \n",
    "from numba import njit, prange\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "is_train = True  \n",
    "is_infer = True  \n",
    "split_day = 435  # Split day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1423e7-8f37-4124-aab7-e36b54c06c8e",
   "metadata": {},
   "source": [
    "## Data Loading and Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "363c5b19-9d16-4f90-84c9-db1480930596",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"optiver-trading-at-the-close/train.csv\")\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df_shape = df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39cad332-7aab-4904-a946-f2717d9659be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#memory reduction\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    print(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "    decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "    print(f\"Decreased by {decrease:.2f}%\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ed6b286-f57d-4bc8-90c4-8c0ac2d43aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute imbalance \n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "   \n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "    \n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be4e5f24-8649-40ab-be24-4581fff3f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get new features \n",
    "\n",
    "def imbalance_features(df):\n",
    "    \n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "\n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"ask_price + bid_price\")/2\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"imbalance_size-matched_size\")/df.eval(\"matched_size+imbalance_size\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "    \n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "        \n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    \n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "    \n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size','market_urgency', 'imbalance_momentum', 'size_imbalance']:\n",
    "        for window in [1, 2, 3, 10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "\n",
    "    return df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "def numba_imb_features(df):\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    \n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "    return df\n",
    "\n",
    "# embedded time into features \n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5  \n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60 \n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  \n",
    "\n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_all_features(df):\n",
    "  \n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df = df[cols]\n",
    "    df = imbalance_features(df)\n",
    "    df = numba_imb_features(df)\n",
    "    df = other_features(df)\n",
    "    gc.collect() \n",
    "    \n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    \n",
    "    return df[feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e90c9c44-379b-4610-9571-ffee24de0434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define weights for each feature\n",
    "\n",
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02 ,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "\n",
    "weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b023346c-4686-4323-a97f-b458790f9db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Online Train Feats Finished.\n",
      "Memory usage of dataframe is 4955.28 MB\n",
      "Memory usage after optimization is: 2397.72 MB\n",
      "Decreased by 51.61%\n"
     ]
    }
   ],
   "source": [
    "df_train = df\n",
    "\n",
    "if is_train:\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    }\n",
    " \n",
    "    df_train_feats = generate_all_features(df_train)\n",
    "    print(\"Build Online Train Feats Finished.\")\n",
    "    df_train_feats = reduce_mem_usage(df_train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "135be51a-b1a2-4d83-968e-fd2f4a0c4612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stock_id',\n",
       " 'seconds_in_bucket',\n",
       " 'imbalance_size',\n",
       " 'imbalance_buy_sell_flag',\n",
       " 'reference_price',\n",
       " 'matched_size',\n",
       " 'far_price',\n",
       " 'near_price',\n",
       " 'bid_price',\n",
       " 'bid_size',\n",
       " 'ask_price',\n",
       " 'ask_size',\n",
       " 'wap',\n",
       " 'volume',\n",
       " 'mid_price',\n",
       " 'liquidity_imbalance',\n",
       " 'matched_imbalance',\n",
       " 'size_imbalance',\n",
       " 'reference_price_far_price_imb',\n",
       " 'reference_price_near_price_imb',\n",
       " 'reference_price_ask_price_imb',\n",
       " 'reference_price_bid_price_imb',\n",
       " 'reference_price_wap_imb',\n",
       " 'far_price_near_price_imb',\n",
       " 'far_price_ask_price_imb',\n",
       " 'far_price_bid_price_imb',\n",
       " 'far_price_wap_imb',\n",
       " 'near_price_ask_price_imb',\n",
       " 'near_price_bid_price_imb',\n",
       " 'near_price_wap_imb',\n",
       " 'ask_price_bid_price_imb',\n",
       " 'ask_price_wap_imb',\n",
       " 'bid_price_wap_imb',\n",
       " 'imbalance_momentum',\n",
       " 'price_spread',\n",
       " 'spread_intensity',\n",
       " 'price_pressure',\n",
       " 'market_urgency',\n",
       " 'depth_pressure',\n",
       " 'matched_size_shift_1',\n",
       " 'matched_size_ret_1',\n",
       " 'matched_size_shift_2',\n",
       " 'matched_size_ret_2',\n",
       " 'matched_size_shift_3',\n",
       " 'matched_size_ret_3',\n",
       " 'matched_size_shift_10',\n",
       " 'matched_size_ret_10',\n",
       " 'imbalance_size_shift_1',\n",
       " 'imbalance_size_ret_1',\n",
       " 'imbalance_size_shift_2',\n",
       " 'imbalance_size_ret_2',\n",
       " 'imbalance_size_shift_3',\n",
       " 'imbalance_size_ret_3',\n",
       " 'imbalance_size_shift_10',\n",
       " 'imbalance_size_ret_10',\n",
       " 'reference_price_shift_1',\n",
       " 'reference_price_ret_1',\n",
       " 'reference_price_shift_2',\n",
       " 'reference_price_ret_2',\n",
       " 'reference_price_shift_3',\n",
       " 'reference_price_ret_3',\n",
       " 'reference_price_shift_10',\n",
       " 'reference_price_ret_10',\n",
       " 'imbalance_buy_sell_flag_shift_1',\n",
       " 'imbalance_buy_sell_flag_ret_1',\n",
       " 'imbalance_buy_sell_flag_shift_2',\n",
       " 'imbalance_buy_sell_flag_ret_2',\n",
       " 'imbalance_buy_sell_flag_shift_3',\n",
       " 'imbalance_buy_sell_flag_ret_3',\n",
       " 'imbalance_buy_sell_flag_shift_10',\n",
       " 'imbalance_buy_sell_flag_ret_10',\n",
       " 'ask_price_diff_1',\n",
       " 'ask_price_diff_2',\n",
       " 'ask_price_diff_3',\n",
       " 'ask_price_diff_10',\n",
       " 'bid_price_diff_1',\n",
       " 'bid_price_diff_2',\n",
       " 'bid_price_diff_3',\n",
       " 'bid_price_diff_10',\n",
       " 'ask_size_diff_1',\n",
       " 'ask_size_diff_2',\n",
       " 'ask_size_diff_3',\n",
       " 'ask_size_diff_10',\n",
       " 'bid_size_diff_1',\n",
       " 'bid_size_diff_2',\n",
       " 'bid_size_diff_3',\n",
       " 'bid_size_diff_10',\n",
       " 'market_urgency_diff_1',\n",
       " 'market_urgency_diff_2',\n",
       " 'market_urgency_diff_3',\n",
       " 'market_urgency_diff_10',\n",
       " 'imbalance_momentum_diff_1',\n",
       " 'imbalance_momentum_diff_2',\n",
       " 'imbalance_momentum_diff_3',\n",
       " 'imbalance_momentum_diff_10',\n",
       " 'size_imbalance_diff_1',\n",
       " 'size_imbalance_diff_2',\n",
       " 'size_imbalance_diff_3',\n",
       " 'size_imbalance_diff_10',\n",
       " 'all_prices_mean',\n",
       " 'all_sizes_mean',\n",
       " 'all_prices_std',\n",
       " 'all_sizes_std',\n",
       " 'all_prices_skew',\n",
       " 'all_sizes_skew',\n",
       " 'all_prices_kurt',\n",
       " 'all_sizes_kurt',\n",
       " 'ask_price_bid_price_wap_imb2',\n",
       " 'ask_price_bid_price_reference_price_imb2',\n",
       " 'ask_price_wap_reference_price_imb2',\n",
       " 'bid_price_wap_reference_price_imb2',\n",
       " 'matched_size_bid_size_ask_size_imb2',\n",
       " 'matched_size_bid_size_imbalance_size_imb2',\n",
       " 'matched_size_ask_size_imbalance_size_imb2',\n",
       " 'bid_size_ask_size_imbalance_size_imb2',\n",
       " 'dow',\n",
       " 'seconds',\n",
       " 'minute',\n",
       " 'global_median_size',\n",
       " 'global_std_size',\n",
       " 'global_ptp_size',\n",
       " 'global_median_price',\n",
       " 'global_std_price',\n",
       " 'global_ptp_price']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_feats.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68d466-0356-4b68-82f6-440168e9c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_params = {\n",
    "#     \"objective\": \"mae\",\n",
    "#     \"n_estimators\": 5500,\n",
    "#     \"num_leaves\": 128,\n",
    "#     \"subsample\": 0.6,\n",
    "#     \"colsample_bytree\": 0.8,\n",
    "#     \"learning_rate\": 0.00005,\n",
    "#     'max_depth': 11,\n",
    "#     \"n_jobs\": 4,\n",
    "#     \"device\": \"gpu\",\n",
    "#     \"verbosity\": -1,\n",
    "#     \"importance_type\": \"gain\",\n",
    "# }\n",
    "\n",
    "# fine-tuned\n",
    "lgb_params = {\n",
    "    \"objective\": \"mae\",\n",
    "    \"n_estimators\": 6000,\n",
    "    \"num_leaves\": 256,\n",
    "    \"subsample\": 0.6,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"learning_rate\": 0.00871,\n",
    "    'max_depth': 11,\n",
    "    \"n_jobs\": 4,\n",
    "    \"device\": \"gpu\",\n",
    "    \"verbosity\": -1,\n",
    "    \"importance_type\": \"gain\",\n",
    "}\n",
    "\n",
    "feature_name = list(df_train_feats.columns)\n",
    "print(f\"Feature length = {len(feature_name)}\")\n",
    "\n",
    "#cross-validation parameters\n",
    "num_folds = 5\n",
    "fold_size = 480 // num_folds\n",
    "gap = 5\n",
    "\n",
    "models = []\n",
    "scores = []\n",
    "\n",
    "model_save_path = f'/kaggle/working/BaseModel/';\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "date_ids = df_train['date_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc4efc6-d125-425f-b180-d5c52e2f160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "for i in range(num_folds):\n",
    "    start = i * fold_size\n",
    "    end = start + fold_size\n",
    "    if i < num_folds - 1:  \n",
    "        purged_start = end - 2\n",
    "        purged_end = end + gap + 2\n",
    "        train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n",
    "    else:\n",
    "        train_indices = (date_ids >= start) & (date_ids < end)\n",
    "    \n",
    "    test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n",
    "    \n",
    "    df_fold_train = df_train_feats[train_indices]\n",
    "    df_fold_train_target = df_train['target'][train_indices]\n",
    "    df_fold_valid = df_train_feats[test_indices]\n",
    "    df_fold_valid_target = df_train['target'][test_indices]\n",
    "    print(f\"Fold {i+1} Model Training\")\n",
    "    \n",
    "    lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "    lgb_model.fit(\n",
    "        df_fold_train[feature_name],\n",
    "        df_fold_train_target,\n",
    "        eval_set=[(df_fold_valid[feature_name], df_fold_valid_target)],\n",
    "        callbacks=[\n",
    "            lgb.callback.early_stopping(stopping_rounds=100),\n",
    "            lgb.callback.log_evaluation(period=100),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    models.append(lgb_model)\n",
    "    \n",
    "    # Save model\n",
    "    model_filename = os.path.join(model_save_path, f'lbg_mdl_{i+1}.txt')\n",
    "    lgb_model.booster_.save_model(model_filename)\n",
    "    print(f\"Model for fold {i+1} saved to {model_filename}\")\n",
    "\n",
    "    fold_predictions = lgb_model.predict(df_fold_valid[feature_name])\n",
    "    fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n",
    "    scores.append(fold_score)\n",
    "    print(f\"Fold {i+1} MAE: {fold_score}\")\n",
    "\n",
    "    del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ce50dd-d88f-4af2-bd0a-628c52b306b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n",
    "\n",
    "final_model_params = lgb_params.copy()\n",
    "final_model_params['n_estimators'] = average_best_iteration\n",
    "\n",
    "print(f\"Training final model with average best iteration: {average_best_iteration}\")\n",
    "\n",
    "final_model = lgb.LGBMRegressor(**final_model_params)\n",
    "final_model.fit(\n",
    "    df_train_feats[feature_name],\n",
    "    df_train['target'],\n",
    "    callbacks=[\n",
    "        lgb.callback.log_evaluation(period=100),\n",
    "    ],\n",
    ")\n",
    "\n",
    "models.append(final_model)\n",
    "\n",
    "final_model_filename = os.path.join(model_save_path, 'lgbmdl.txt')\n",
    "final_model.booster_.save_model(final_model_filename)\n",
    "print(f\"Final model saved to {final_model_filename}\")\n",
    "\n",
    "print(f\"Average MAE: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f84690d-9127-4b2d-8855-78b9656630bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)  # Calculate standard error based on volumes\n",
    "    step = np.sum(prices) / np.sum(std_error)  #  Calculate the step size based on prices and standard error\n",
    "    out = prices - std_error * step  #  Adjust prices by subtracting the standardized step size\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf46150-936f-4b4b-918d-7b72b552c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "import optiver2023\n",
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()\n",
    "counter = 0\n",
    "y_min, y_max = -64, 64\n",
    "qps, predictions = [], []\n",
    "cache = pd.DataFrame()\n",
    "\n",
    "#add weights\n",
    "model_weights = [1/len(models)] * len(models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a44d48-2c08-41a8-a8fb-50c062c4a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run test \n",
    "for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "    now_time = time.time()\n",
    "    test.drop('currently_scored', axis=1, inplace=True)\n",
    "    cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "\n",
    "    if counter > 0:\n",
    "        cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "\n",
    "    # Generate features \n",
    "    feat = generate_all_features(cache)[-len(test):]\n",
    "\n",
    "    # Make predictions \n",
    "    lgb_predictions = np.zeros(len(test))\n",
    "    for model, weight in zip(models, model_weights):\n",
    "        lgb_predictions += weight * model.predict(feat)\n",
    "\n",
    "    # Adjust predictions \n",
    "    lgb_predictions = zero_sum(lgb_predictions, test['bid_size'] + test['ask_size'])\n",
    "    clipped_predictions = np.clip(lgb_predictions, y_min, y_max)  \n",
    "    sample_prediction['target'] = clipped_predictions\n",
    "    env.predict(sample_prediction)  # Submit\n",
    "    counter += 1\n",
    "    qps.append(time.time() - now_time)\n",
    "\n",
    "    if counter % 10 == 0:\n",
    "        print(counter, 'qps:', np.mean(qps))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
